
<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
	<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Santosh's blog &mdash; K Nearest Neighbors</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Free HTML5 Website Template by FreeHTML5.co" />
	<meta name="keywords" content="free html5, free template, free bootstrap, free website template, html5, css3, mobile first, responsive" />
	<meta name="author" content="FreeHTML5.co" />


  	<!-- Facebook and Twitter integration -->
	<meta property="og:title" content=""/>
	<meta property="og:image" content=""/>
	<meta property="og:url" content=""/>
	<meta property="og:site_name" content=""/>
	<meta property="og:description" content=""/>
	<meta name="twitter:title" content="" />
	<meta name="twitter:image" content="" />
	<meta name="twitter:url" content="" />
	<meta name="twitter:card" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<link rel="shortcut icon" href="favicon.ico">

	<link href='https://fonts.googleapis.com/css?family=Roboto:400,300,600,400italic,700' rel='stylesheet' type='text/css'>
	<link href='https://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>
	<link href="https://fonts.googleapis.com/css?family=Quicksand" rel="stylesheet" type="text/css">
	<!-- Animate.css -->
	<link rel="stylesheet" href="css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="css/bootstrap.css">
	<!-- Owl Carousel -->
	<link rel="stylesheet" href="css/owl.carousel.min.css">
	<link rel="stylesheet" href="css/owl.theme.default.min.css">
	<!-- Theme style  -->
	<link rel="stylesheet" href="css/style.css">

	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	<!-- Script to integrate TeX in html-->
	<script type="text/x-mathjax-config">
	  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script>
	<script type="text/x-mathjax-config">
		MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }
});
	</script>
	<script type="text/javascript"
	  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
<script type="text/javascript" src="http://latex.codecogs.com/latexit.js"></script>
	</head>
	<body>

	<div id="fh5co-page">
		<a href="#" class="js-fh5co-nav-toggle fh5co-nav-toggle"><i></i></a>
		<aside id="fh5co-aside" role="complementary" class="border js-fullheight">

			<h1 id="fh5co-logo"><a href="index.html"><img src="images/logo.png" alt="Free HTML5 Bootstrap Website Template"></a></h1>
			<nav id="fh5co-main-menu" role="navigation">
				<ul>
					<li><a href="index.html">Home</a></li>
					<li><a href="portfolio.html">Portfolio</a></li>
					<li><a href="about.html">About</a></li>
					<li><a href="contact.html">Contact</a></li>
				</ul>
			</nav>

			<div class="fh5co-footer">
				<p><small>&copy; 2016 Nitro Free HTML5. All Rights Reserved.</span> <span>Designed by <a href="http://freehtml5.co/" target="_blank">FreeHTML5.co</a> </span> <span>Demo Images: <a href="http://unsplash.com/" target="_blank">Unsplash</a></span></small></p>
				<ul>
					<li><a href="#"><i class="icon-facebook"></i></a></li>
					<li><a href="#"><i class="icon-twitter"></i></a></li>
					<li><a href="#"><i class="icon-instagram"></i></a></li>
					<li><a href="#"><i class="icon-linkedin"></i></a></li>
				</ul>
			</div>

		</aside>

		<div id="fh5co-main">

			<div class="fh5co-narrow-content">

					<div class="col-md-12 animate-box">
							<h1>K Nearest Neighbor (KNN)</h1>
							Here are the list of things covered in this post.
							<ul>
								<li>Introduction</li>
								<li>Types</li>
								<li>Algorithm</li>
								<li>Finding optimal <em>k</em></li>
								<li>Distance Metrics</li>
								<li>Time and space analysis</li>
								<li>Limitations</li>
								<li>KD Trees</li>
								<li>Locality Sensitive Hashing</li>
							</ul>
					</div>

					<div class="col-md-12 animate-box">
							<h3 class="up-one">Classification and Regression with KNN</h3>
							<ol>
								<li>In K-NN classification, we consider the closest <em>k</em> neighbors to the given data point and assign the class label of the majority class in those k neighbors to the given data point.
								<ul>
									<li>For example, let there are 10 data points, $x_1$ through $x_{10}$ that are classified into two classes $class_1$ and $class_2$ as shown in the figure below. Assume $k = 5$, the query point be $\langle x_q, ?\rangle $ and
										$\langle x_1, class_1\rangle $,
										$\langle x_2, class_1\rangle $,
										$\langle x_3, class_2\rangle $,
										$\langle x_4, class_1\rangle $,
										$\langle x_5, class_1\rangle $,
									be the 5 points closest to $x_q$. We see that the majority of the points have been classified as $class_1$. So, the query point $x_q$ is to be classified as $class_1$. <br>
									<p style="text-align:center;">
										<img src="images/knn_intuition.png" alt="KNN Intuition">
									</p>
								</li>

							</ul>
							</li>
							<li>In K-NN Regression, we find the output value of a query data point by averaging the outputs of its k closest datapoints.</li>
							</ol>
							<div class="col-md-12 animate-box">
								<h3 class="up-one">Problem Definition</h3>
								<p>For simplicity, let us assume that we are dealing with binary class classification problem where the data $D$ is
								$$\mathcal{D} = \{x_i, y_i\}_{i=1}^n$$
								where $x_i \in \mathbb{R}^d$, $y_i\in \{0,1\}, \forall i \in n$. For a query datapoint $x_q$, we have to classify it as either 0 or 1.</p>
								<p>Before going to the actual algorithm, lets look at some of the preliminaries on distance metrics that would require attention for understanding the KNN learning algorithm.</p>
							</div>

							<div class="col-md-12 animate-box">
							<h3 id="distance">Distance Metrics</h3>
							<p>In the KNN algorithm, we find the similarity/proximity/closeness between two points $x$ and $x'$ which can be defined using some distance metric $d(x,x')$. The distance metric can be Euclidean distance or Manhattan distance or Chebyshev distance or any other similarity measurement which can capture the closeness between two points.</p>
							<ol>
								<li><u>Euclidean distance:</u> <br>
									The Euclidean distance between two points $p$ and $q$ is the length of the line segment connecting them. <br>
									In the 2-dimensional space, given the points $p = (p_1, p_2)$ and $q = (q_1, q_2)$, the Euclidean distance $d(p,q)$ from $p$ to $q$ is given by the Pythogorean formula:
									$$d(p,q) = \sqrt{(p_1-q_1)^2+(p_2-q_2)^2}$$
									Similarly, in an n-dimensional space, given the points $p = (p_1, p_2, \ldots, p_n)$ and $q = (q_1, q_2, \ldots, q_n)$, the Euclidean distance $d(p,q)$ from $p$ to $q$ is given by the Pythogorean formula:
									$$d_{euclidean}(p,q) = \sqrt{(p_1-q_1)^2+(p_2-q_2)^2+\ldots+(p_n-q_n)^2} = \sqrt{\sum_{i=1}^n (p_i-q_i)^2} $$
								</li>

								<li><u>Manhattan distance:</u><br>
									The Manhattan distance between two points $p$ and $q$ is the sum of absolute difference of the coordinates. For example, in a 2D plane, the Manhattan distance between $p(x_1, y_1)$ and $q(x_2,y_2)$ is given by
									$$d(p,q) = |x_2 - x_1|+|y_2-y_1|$$
									This is also called the $L_1$ distance. Below is the figure which gives you an intuition of finding Manhattan distance in 2D space.
									<p style="text-align:center;">
									<img src="images/manhattan.png" align="middle"><p>
									Similarly, in an n-dimensional space, the Manhattan or $L_1$ distance between two points $p = (p_1, p_2, \ldots, p_n)$ and $q = (q_1, q_2, \ldots, q_n)$ is given by:
									$$
									d_{manhattan}(p,q) = |p_1-q_1|+|p_2-q_2|+\ldots+|p_n-q_n| = \sum_{i=1}^n |p_i-q_i| $$
								</li>

								<li><u>Chebyshev distance:</u><br>
									In contrast to Manhattan distance where we take the sum of the absolute differences along the coordinates, in Chebyshev distance metric we consider maximum of the absolute differences along the coordinates.<br>
									In a 2D plane, the Chebyshev distance between $p(x_1, y_1)$ and $q(x_2,y_2)$ is
									$$d(p,q) = \max{\{|x_2 - x_1|, |y_2-y_1|\}}$$
									In an n-dimensional space, the Chebyshev distance between two points $p = (p_1, p_2, \ldots, p_n)$ and $q = (q_1, q_2, \ldots, q_n)$ is given by:
									$$d_{chebyshev}(p,q) = \max\{{|p_1 -q_1|, |p_2-q_2|,\ldots,|p_n-q_n|}\} = \max_{i=1}^n{\{|p_i -q_i|\}}
									$$
								</li>
							</ol>
							<p>We have learnt about the distance metrics, we shall look at the algorithm.</p>
						</div>

						<div class="col-md-12 animate-box">
								<h3>Bruteforce Algorithm for KNN</h3>
								<p>For simplicity, we shall look at KNN classifier with two classes. Given a dataset $D = \{x_i, y_i\}_{i=0}^n$, where $x_i\in\mathbb{R}^d$, $y_i=\{0,1\}$ and a query point $x_q$, we need to find the class which $x_q$ belongs to. Assume that $k$ is given.</p>
								<ol>
									<li>For each datapoint $x_i\in \mathcal{D}$, compute the distance $d(x_i, x_q)$. We can use any metric discussed <a href="#distance">above</a>. Mostly we will be using Euclidean distance as our distance metric.</li>
									<li>Sort the dataset in non-decreasing order of the distances computed in step 1. Pick top $k$ datapoints which are nearest to $x_q$.
										<div class="alert alert-block alert-info"><em>We don't actually need to sort the entire dataset. We can just select the indices of $k$ nearest points by iterating through the data $k$ times. This reduces the time complexity from O(n log n) to O(kn), and k is always small. Infact, the time complexity of the whole algorithm depends especially on this step. An efficient implementation of this step reduces the runtime significantly.</em></div>
									</li>
									<li>Majority vote on the $k$ points and assign the corresponding label to $x_q$.</li>
								</ol>
								<p>The algorithm is so simple and elegant, we just need to compute the distances from each datapoint to the query point and assign the majority label of k nearest datapoints.</p>
							</div>

							<div class="col-md-12 animate-box">
							<h3>Characteristics of KNN</h3>
							<ol>
								<li>KNN is a <a href="https://en.wikipedia.org/wiki/Nonparametric_statistics">non-parametric</a> statistic algorithm. It is non-parametric because we do not make any assumption on the underlying distribution of the dataset. Non-parametric methods does not have fixed number of parameters or estimates of the parameters in the model.
									<div class="alert alert-block alert-success"><em>Non-parametric</em> statistics are statistics not based on parameterized families of probability distributions. They include both descriptive and inferential statistics. The typical parameters are the mean, variance, etc. <br>
									<em>Unlike parametric statistics, nonparametric statistics make no assumptions about the probability distributions of the variables being assessed. The difference between parametric models and non-parametric models is that the former has a fixed number of parameters, while the latter grows the number of parameters with the amount of training data.</em> Note that the non-parametric model does, counterintuitively, contain parameters: the distinction is that parameters are determined by the training data in the case of non-parametric statistics, not the model.
									</div>
									From the above discussion, we can see that our parameter in this algorithm is $k$, called the <em>hyper-parameter</em>.
								</li>
								<li><p>KNN is a <a href="https://en.wikipedia.org/wiki/Lazy_learning">lazy learning</a> algorithm because it does not learn a discriminative function from the data, but instead it memorizes the training data. Intuitively, we do not have a training phase in this algorithm. Instead, we find the distances from $x_q$ to each $x_i \in D$ whenever we have a new query point $x_q$. </p>
									<div class="alert alert-block alert-success">For example, the logistic regression algorithm learns its model weights (parameters) during training time. In contrast, there is no training time in KNN. Although this may sound very convenient, this property doesn’t come without a cost: The “prediction” step in KNN is relatively expensive! Each time we want to make a prediction, KNN is searching for the nearest neighbor(s) in the entire training set! (Note that there are certain tricks such as BallTrees and KDtrees to speed this up a bit.)
									</div>
									To summarize, an eager learner has a model fitting or training step. A lazy learner does not have a training phase.
								</li>
							</ol>
						</div>

						<div class="col-md-12 animate-box">
							<h3 class="up-one">Analysis of KNN</h3>
							<ol>
								<li>
									Since KNN does not have a training phase (remember, K-NN is a lazy learner), it does not construct a model or a discriminative function from the data. So, the time taken for model construction is absolutely 0. And hence the space required for model construction is also 0.
								</li>
								<li>
									During the runtime or testing phase, given a query point $x_q$, computing the distance from a datapoint in $\mathcal{D}$ to $x_q$ takes O(d) runtime. For n such computations, time taken is O(nd). Once we get all the distances,  we can just select the indices of $k$ nearest points by iterating through the dataset $k$ times. This takes O(kn) time (and k is always small). So, the overall time complexity in the testing phase is O(nd+kn). We can use a modification of <a href="https://en.wikipedia.org/wiki/Median_of_medians">quickselect algorithm</a> to find the $k^{th}$ percentile in O(n) time, which reduces the overall time complexity to O(nd+n). The space complexity is O(n), since we need to store all the data in the memory.
								</li>
							</ol>
						</div>

						<div class="col-md-12 animate-box">
									<p>In the above algorithm, we assumed that $k$ is known. But in real world applications, we do not know what $k$ is and we need to choose an optimal (or approximately good estimate) of $k$. For that we introduce a concept called "cross-validation".
									So far so good! Let us see what are the limitations of this algorithm.</p>
						</div>

						<div class="col-md-12 animate-box">
							<h3>Limitations</h3>
							<ol>
								<li>
									Since KNN is a lazy learner, it does not learn anything from the given data. Instead, it uses the training data for prediction. This can be slow for real-time prediction if there are a large number of training examples. We can use some indexing methods like KDTrees to reduce the runtime.
								</li>
								<li>
									We need to find the optimal value of $k$ using some hyper-parameter tuning methods like cross-validation.
								</li>
							</ol>
						</div>
						<div class="col-md-3 col-md-pull-9 fh5co-services">
							<h3>Services</h3>
							<ul>
								<li>Web Design</li>
								<li>e-Commerce</li>
								<li>Logo &amp; Branding</li>
								<li>Packaging</li>
							</ul>
						</div>

					</div>
				</div>

				<div class="row work-pagination animate-box" data-animate-effect="fadeInLeft">
					<div class="col-md-8 col-md-offset-2 col-sm-12 col-sm-offset-0">

						<div class="col-md-4 col-sm-4 col-xs-4 text-center">
							<a href="#"><i class="icon-long-arrow-left"></i> <span>Previous Project</span></a>
						</div>
						<div class="col-md-4 col-sm-4 col-xs-4 text-center">
							<a href="#"><i class="icon-th-large"></i></a>
						</div>
						<div class="col-md-4 col-sm-4 col-xs-4 text-center" align="right">
							<a href="#"><span>Next Project</span> <i class="icon-long-arrow-right"></i></a>
						</div>
					</div>
				</div>


	<!-- jQuery -->
	<script src="js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script src="js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="js/bootstrap.min.js"></script>
	<!-- Carousel -->
	<script src="js/owl.carousel.min.js"></script>
	<!-- Stellar -->
	<script src="js/jquery.stellar.min.js"></script>
	<!-- Waypoints -->
	<script src="js/jquery.waypoints.min.js"></script>
	<!-- Counters -->
	<script src="js/jquery.countTo.js"></script>


	<!-- MAIN JS -->
	<script src="js/main.js"></script>

	</body>
</html>
